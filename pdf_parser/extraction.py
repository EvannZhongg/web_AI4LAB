# evannzhongg/ai4mw_web/AI4MW_Web-b75f2e933ce5eb3d7c9b77393d2d6eec787f7611/pdf_parser/extraction.py

import os
import json
import logging
from time import sleep
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI, OpenAIError
from pathlib import Path
from typing import List, Dict, Any, Set
from .prompts import get_model_extraction_prompt

logger = logging.getLogger(__name__)


# === è¾…åŠ©å‡½æ•°: æ–‡æœ¬éªŒè¯ ===
def validate_model_in_text(model_name, text):
    """
    éªŒè¯ model_name æ˜¯å¦å­˜åœ¨äº text ä¸­ã€‚
    æ”¯æŒå¿½ç•¥ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ #ï¼‰ã€‚
    """
    # å»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦åè¿›è¡ŒåŒ¹é…
    normalized_model = model_name.replace(" ", "").replace("#", "")
    normalized_text = text.replace(" ", "").replace("#", "")
    return normalized_model in normalized_text


# === è¾…åŠ©å‡½æ•°: ç´§å‡‘ JSON ===
def custom_json_dump(obj, file):
    """è‡ªå®šä¹‰æ ¼å¼åŒ–è¾“å‡ºï¼šä¿ç•™æ•´ä½“ç¼©è¿›ï¼Œä½†è®© chunk_ids ç´§å‡‘æ’åˆ—"""
    formatted_output = []
    for item in obj:
        model_name = item["model_name"]
        # ä¿®å¤ï¼šç¡®ä¿ chunk_ids æ˜¯æ•°å­—ï¼Œç„¶åè½¬ä¸ºå­—ç¬¦ä¸²
        chunk_ids_str = f"[{','.join(map(str, item['chunk_ids']))}]"
        formatted_output.append(f'  {{"model_name": "{model_name}", "chunk_ids": {chunk_ids_str}}}')
    file.write("[\n" + ",\n".join(formatted_output) + "\n]")


# === å†…éƒ¨ LLM è°ƒç”¨å‡½æ•° (Pass 1) ===
def _call_llm_for_extraction(
        client: OpenAI,
        model_config: Dict[str, Any],
        chunk: Dict[str, Any],
        max_retries: int
) -> Dict[str, Any]:
    """
    (Pass 1)
    ä»…è´Ÿè´£è°ƒç”¨ LLM å¹¶è¿”å›åŸå§‹ç»“æœï¼Œå¤„ç† API å’Œ JSON çº§åˆ«çš„é‡è¯•ã€‚
    ä¸è¿›è¡Œä¸šåŠ¡é€»è¾‘éªŒè¯ã€‚
    """
    chunk_id = chunk["id"]
    text = chunk["text"]
    user_prompt = get_model_extraction_prompt()

    for attempt in range(1, max_retries + 1):
        try:
            response = client.chat.completions.create(
                model=model_config["model"],
                messages=[
                    {"role": "system",
                     "content": f"ä»¥ä¸‹æ˜¯ç”µå­å™¨ä»¶æ•°æ®æ‰‹å†Œä¸­çš„ä¸€æ®µ Markdown æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªå‹å·çš„åç§°ï¼Œæˆ–ä»…ä»…æ˜¯å…¬å…±ä¿¡æ¯ï¼‰ï¼Œç”¨äºåˆ†æï¼š\n{text}"},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=model_config["temperature"],
                stream=False,
                response_format={"type": "json_object"}
            )
            output_text = response.choices[0].message.content.strip()

            usage = response.usage
            token_usage = {
                "prompt": usage.prompt_tokens,
                "completion": usage.completion_tokens,
            }

            # å°è¯•è§£æ JSON
            result = json.loads(output_text)

            # API è°ƒç”¨å’Œ JSON è§£æå‡æˆåŠŸ
            return {
                "chunk_id": chunk_id,
                "text": text,
                "llm_output": result,
                "token_usage": token_usage,
                "status": "success"
            }

        except json.JSONDecodeError:
            logger.warning(f"Chunk {chunk_id} ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: éæ³• JSON æ ¼å¼: {output_text}")
            if attempt < max_retries:
                logger.info(f"ğŸ”„ Chunk {chunk_id} å°è¯•é‡æ–°å¤„ç† (JSON é”™è¯¯)...")
                sleep(3)  # ç­‰å¾…åé‡è¯•
            else:
                logger.error(f"âŒ Chunk {chunk_id} æœ€ç»ˆå¤±è´¥: éæ³• JSON æ ¼å¼")
                return {"chunk_id": chunk_id, "text": text, "llm_output": None,
                        "token_usage": {"prompt": 0, "completion": 0}, "status": "json_error"}

        except OpenAIError as e:
            logger.warning(f"âš ï¸ Chunk {chunk_id} ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {e}")
            if attempt < max_retries:
                sleep(3)  # ç­‰å¾…åé‡è¯•
            else:
                logger.error(f"âŒ Chunk {chunk_id} æœ€ç»ˆå¤±è´¥: OpenAI é”™è¯¯")
                return {"chunk_id": chunk_id, "text": text, "llm_output": None,
                        "token_usage": {"prompt": 0, "completion": 0}, "status": "api_error"}

    # å¾ªç¯ç»“æŸï¼ˆç†è®ºä¸Šä¸åº”åˆ°è¾¾è¿™é‡Œï¼‰
    return {"chunk_id": chunk_id, "text": text, "llm_output": None, "token_usage": {"prompt": 0, "completion": 0},
            "status": "unknown_error"}


# --- ä¸»åè°ƒå‡½æ•° (ä¾› Celery è°ƒç”¨) ---
def process_chunks_for_model_extraction(
        basic_chunk_path: str,
        results_dir: str,
        llm_config: Dict[str, Any],
        extraction_config: Dict[str, Any]
):
    """
    (å·²é‡æ„) ä» Celery è°ƒç”¨çš„ä¸»å‡½æ•°ï¼Œåè°ƒæ‰€æœ‰å‹å·æŠ½å–æ­¥éª¤ã€‚
    é‡‡ç”¨ä¸¤é˜¶æ®µéªŒè¯é€»è¾‘ã€‚
    """
    logger.info(f"å¼€å§‹å‹å·æŠ½å–: {basic_chunk_path}")

    # 1. å‡†å¤‡é…ç½®å’Œ Client
    max_workers = extraction_config.get("MAX_WORKERS", 5)
    max_retries = extraction_config.get("MAX_RETRIES", 3)

    client = OpenAI(
        api_key=llm_config["api_key"],
        base_url=llm_config["base_url"]
    )

    model_config = {
        "model": llm_config["model_name"],
        "temperature": extraction_config.get("TEMPERATURE", 0.0)
    }

    # 2. è¯»å– basic_chunk.json
    try:
        with open(basic_chunk_path, "r", encoding="utf-8") as f:
            chunks = json.load(f)
        if not chunks:
            logger.warning(f"æ–‡ä»¶ {basic_chunk_path} ä¸ºç©ºï¼Œè·³è¿‡å‹å·æŠ½å–ã€‚")
            return
    except Exception as e:
        logger.error(f"æ— æ³•è¯»å– {basic_chunk_path}: {e}")
        raise

    # 3. Pass 1: å¹¶å‘æ‰§è¡Œ LLM è°ƒç”¨
    raw_llm_results = []
    total_prompt_tokens = 0
    total_completion_tokens = 0

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(_call_llm_for_extraction, client, model_config, chunk, max_retries): chunk
            for chunk in chunks
        }

        for future in as_completed(futures):
            raw_result = future.result()
            raw_llm_results.append(raw_result)
            total_prompt_tokens += raw_result["token_usage"]["prompt"]
            total_completion_tokens += raw_result["token_usage"]["completion"]

    # 4. Pass 2: é¦–æ¬¡éªŒè¯å¹¶æ„å»ºâ€œå…¨å±€æœ‰æ•ˆå‹å·â€åˆ—è¡¨
    all_valid_models_in_doc = set()
    chunks_to_revalidate = []
    validated_results = []  # å­˜å‚¨æ‰€æœ‰æœ€ç»ˆé€šè¿‡éªŒè¯çš„ç»“æœ

    logger.info("--- Pass 2a: é¦–æ¬¡éªŒè¯å¼€å§‹ ---")
    for result in raw_llm_results:
        if result["status"] != "success":
            logger.error(f"Chunk {result['chunk_id']} LLM è°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€: {result['status']}ï¼Œå·²è·³è¿‡ã€‚")
            continue

        chunk_id = result["chunk_id"]
        text = result["text"]
        llm_output = result["llm_output"]

        models_found = llm_output.get("models_found", [])
        possible_common = llm_output.get("possible_common", False)

        valid_models_in_chunk = []
        invalid_models_in_chunk = []

        for model_name in models_found:
            if validate_model_in_text(model_name, text):
                valid_models_in_chunk.append(model_name)
                all_valid_models_in_doc.add(model_name)  # æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨
            else:
                invalid_models_in_chunk.append(model_name)

        if not invalid_models_in_chunk:
            # æ­¤å— 100% éªŒè¯é€šè¿‡
            logger.info(f"âœ… Chunk {chunk_id} é¦–æ¬¡éªŒè¯é€šè¿‡ã€‚")
            validated_results.append({
                "chunk_id": chunk_id,
                "models_found": valid_models_in_chunk,
                "possible_common": possible_common
            })
        else:
            # æ­¤å—éœ€è¦è¿›å…¥ Pass 2b é‡æ–°éªŒè¯
            logger.warning(f"âš ï¸ Chunk {chunk_id} é¦–æ¬¡éªŒè¯å¤±è´¥ï¼Œæå–çš„ {invalid_models_in_chunk} æœªåœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°ã€‚")
            chunks_to_revalidate.append({
                "chunk_id": chunk_id,
                "text": text,
                "llm_output": llm_output,
                "valid_models": valid_models_in_chunk,  # å·²éªŒè¯çš„
                "invalid_models": invalid_models_in_chunk  # å¾…äº¤å‰éªŒè¯çš„
            })

    logger.info(f"--- Pass 2b: äº¤å‰éªŒè¯å¼€å§‹ (å…¨å±€æœ‰æ•ˆå‹å·: {all_valid_models_in_doc}) ---")

    for failed_chunk in chunks_to_revalidate:
        chunk_id = failed_chunk["chunk_id"]
        llm_output = failed_chunk["llm_output"]
        final_valid_models = list(failed_chunk["valid_models"])  # ä»å·²éªŒè¯çš„å¼€å§‹

        still_invalid_models = []

        for invalid_model in failed_chunk["invalid_models"]:
            if invalid_model in all_valid_models_in_doc:
                # ä¼˜åŒ–æˆåŠŸï¼šæ¨¡å‹åœ¨å½“å‰å—ä¸å­˜åœ¨ï¼Œä½†åœ¨æ–‡æ¡£åˆ«å¤„å­˜åœ¨
                final_valid_models.append(invalid_model)
            else:
                # çœŸæ­£çš„å¹»è§‰ï¼šæ¨¡å‹åœ¨æ–‡æ¡£ä»»ä½•åœ°æ–¹éƒ½ä¸å­˜åœ¨
                still_invalid_models.append(invalid_model)

        if not still_invalid_models:
            # æ‰€æœ‰æ¨¡å‹éƒ½é€šè¿‡äº†äº¤å‰éªŒè¯
            logger.info(f"âœ… Chunk {chunk_id} äº¤å‰éªŒè¯é€šè¿‡ã€‚")
            validated_results.append({
                "chunk_id": chunk_id,
                "models_found": final_valid_models,
                "possible_common": llm_output.get("possible_common", False)
            })
        else:
            # ä¼˜åŒ–å¤±è´¥ï¼šLLM å½»åº•å¹»è§‰äº†
            logger.error(f"âŒ Chunk {chunk_id} äº¤å‰éªŒè¯å¤±è´¥ã€‚ä»¥ä¸‹å‹å·åœ¨ä»»ä½•åœ°æ–¹éƒ½ä¸å­˜åœ¨: {still_invalid_models}ã€‚")
            # æŒ‰ç…§ç”¨æˆ·è¦æ±‚ï¼Œå°†å…¶è§†ä¸ºå…¬å…±å—ï¼Œåªä¿ç•™åŸå…ˆæœ‰æ•ˆçš„æ¨¡å‹
            validated_results.append({
                "chunk_id": chunk_id,
                "models_found": failed_chunk["valid_models"],  # åªä¿ç•™æœ¬åœ°éªŒè¯çš„
                "possible_common": True  # å¼ºåˆ¶è®¾ä¸º True
            })

    # 5. Pass 3: åˆå¹¶æ‰€æœ‰æœ€ç»ˆé€šè¿‡éªŒè¯çš„ç»“æœ
    logger.info("--- Pass 3: åˆå¹¶æœ€ç»ˆç»“æœ ---")
    merged_models = {}
    common_chunks = []

    for result in validated_results:
        chunk_id = result["chunk_id"]
        models_found = result["models_found"]
        possible_common = result["possible_common"]

        if possible_common:
            common_chunks.append(chunk_id)

        for model_name in models_found:
            if model_name not in merged_models:
                merged_models[model_name] = set()
            merged_models[model_name].add(chunk_id)

    final_model_results = [
        {"model_name": model_name, "chunk_ids": sorted(list(chunk_ids))}
        for model_name, chunk_ids in merged_models.items()
    ]
    final_model_results.sort(key=lambda x: x["model_name"])

    # 6. ä¿å­˜è¾“å‡ºæ–‡ä»¶
    final_model_output_path = Path(results_dir) / "model_chunks.json"
    common_chunks_output_path = Path(results_dir) / "common_chunks.json"

    with open(final_model_output_path, "w", encoding="utf-8") as f:
        custom_json_dump(final_model_results, f)

    with open(common_chunks_output_path, "w", encoding="utf-8") as f:
        json.dump(sorted(common_chunks), f, ensure_ascii=False, separators=(",", ":"))

    logger.info(f"ğŸ‰ å‹å·æŠ½å–ç»“æœå·²ä¿å­˜åˆ°: {final_model_output_path}")
    logger.info(f"ğŸ‰ å…¬å…±å— ID å·²ä¿å­˜åˆ°: {common_chunks_output_path}")
    logger.info(f"ğŸ“Š Token æ¶ˆè€—: Prompt={total_prompt_tokens}, Completion={total_completion_tokens}")